\documentclass[11pt]{amsart}
\usepackage[margin=2cm]{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{url}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}



\begin{document}

\title{IBM ML Prof. Certification: Final Project}
\date{}                                           % Activate to display a given date or no date

\author[J.~Walters]{Jackson Walters}
\address[J.~Walters]{Arlington, VA}
\email{jacksonwalters@gmail.com}
\urladdr{https://jacksonwalters.com}

\maketitle
%\section{}
%\subsection{}

\section{Introduction}

This project is for the Exploratory Data Analysis for Machine Learning certification from IBM via Coursera. The long-term goal is to create a t-SNE plot to observe clustering of mental health disorders based on symptom data, as in the linked paper\footnote{\url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6783390/}}. I would like to include additional data based on life factors such as income, housing status, and insurance status. The idea is that if someone has no income, is unhoused, and uninsured then they would be much more likely to be classified as having a mental illness. I want to go beyond correlation, and see clustering. \\


\section{PCA and t-SNE for MNIST sample data}

For now, I implemented code from here \footnote{\url{https://medium.com/@violante.andre/an-introduction-to-t-sne-with-python-example-47e6ae7dc58f}}, and removed the SAS connection. I rewrote it in pure Python with sklearn. I did both PCA and t-SNE on the MNIST data, and plots are shown below. \\

All code is publicly available on GitHub \footnote{\url{https://github.com/jacksonwalters/ml-examples}}.

\begin{figure}[h]
\caption{PCA plot of MNIST handwritten digits data}
\centering
\includegraphics[width=0.5\textwidth]{PCA_plot_MNIST}
\end{figure}

\begin{figure}[h]
\caption{t-SNE plot of MNIST handwritten digits data}
\centering
\includegraphics[width=0.5\textwidth]{t-SNE_plot_MNIST}
\end{figure}

\section{Mental Health Client-Level Data}

The data are 6.5mil rows and around 50 columns. There are 13 disorder codes listed, each a binary variable. The rest are life factors including employment, demographics like age, gender, ethnicity, geographic information, and more. I have dropped the irrelevant columns, one-hot encoded the categorical columns, and scaled all columns to be in $[0,1]$. \\

There are a few different ways to think of the labels, but they are all derived from the 13 binary diagnosis columns. This is an example of a multi-label problem, distinct from a multi-output or multi-class problem, though we may transform it into one of these. For example, in one labeling I bin the disorders as [\#disorders = 0, \#disorder = 1, \#disorders $> 1$] giving 15 labels. \\

Another approach is to use a binary encoding so that each 13-tuple of binaries maps to a unique integer between 0-8191 since $2^{13} = 8192$. One could also count the number of disorders, yielding 13 labels, since the maximum sum is 13. Finally, one could use a more sophisticated approach like k-means, which finds a centroid of clusters and expands outwards. This is what we will typically use to color the t-SNE and PCA plots: \\

\begin{figure}[h]
\caption{t-SNE plot of mental health data with k-means labeling}
\centering
\includegraphics[width=0.5\textwidth]{t-sne_k-means_label.png}
\end{figure}

Here we choose $num_clusters=15$, and we can clearly see some clusters forming, likely from the demographic information but the diagnostic information is included. If we try to label with $unique_disorders$, that is 13 categories for those diagnosed with a unique disorder and two for no disorders and multi-disorder, we obtain the following plot:

\begin{figure}[h]
\caption{t-SNE plot of mental health data with $unique_disorder$ labeling}
\centering
\includegraphics[width=0.5\textwidth]{tsne-plot_unique-disorders_label.png}
\end{figure}

\section{Classification}

We can also attempt the classification problem. That is, given the demographic and life factor data, predict which disorder class the patient falls into. Since we have a number of labelings, we have a number of different problems to consider. 

First we try the unique\_disorders labeling and train a RandomForest classifier. This is a multi-class problem. \\

\noindent accuracy: 0.34965 \\
precision: 0.30545804698445045 \\
recall: 0.34965 \\

Next up we try multi-class LogisticRegression. On the unique\_disorders labeling, performance is similar to RandomForest. However, on the k-means labeling, we achieve much better performance: \\

\noindent accuracy: 0.88315 \\
precision: 0.8819518553140321 \\
recall: 0.88315 \\

This reflects the much better clustering observed with the k-means labeling. Now the demographic and life factors are able to predict which k-means cluster a point will fall into, without diagnostic information. \\

We can try to drop 12/13 disorder columns and just do a logistic regression on the remaining column. This gives 13 separate logistic regressions, one for each disorder. We can then use it as a mutli-label classification, and compute the Hamming loss (a metric for distance between 0/1 datasets). In this case, we obtain hamming\_loss=0.12. \\

Using MultiOutputClassifier or OneVsRestClassifier, we obtain hamming\_loss=.1 in both cases. Using k-nearest-nearbors MLkNN from skmultilearn.adapt, we obtain hamming\_loss=.11. \\

Finally, using MultiOutputClassifier with XGBoost, we obtain hamming\_loss=.1. The accuracy score is modified a bit to "subset", and reads: \\

\noindent accuracy: 17.2\% \\
precision: 46.8\% \\
recall: 19.8\% \\

It's worth noting that just computing the number of multi-labels that are correctly predicted with XGBoost is ~3\%, whereas a random guess would be ~1/8192 = .01 per data point, so ~12.2\%.

\section{Unsupervised Learning}

I decided to look at the disorder\_cols of the data, disregarding all other columns. This is a Boolean matrix with a lot of structure to uncover. I used a number of clustering algorithms to try to understand which combinations of disorders were most common. I used Hierarchical Agglomerative Clustering, DBSCAN, and k-means. I used non-negative matrix factorization, PCA, and t-SNE for dimensionality reduction.

\begin{figure}[h]
\caption{clustering results using DBSCAN}
\centering
\includegraphics[width=0.5\textwidth]{clustering_techniques.png}
\end{figure}

\begin{figure}[h]
\caption{matrix factorization plot of $H$}
\centering
\includegraphics[width=0.5\textwidth]{matrix_factorization.png}
\end{figure}

\section{Deep Learning and Reinforcement Learning}

I implemented a three layer deep neural network to perform classification on the 14 diagnostic boolean labels. The output of the classifier is 14 numbers which are then rounded to 0 or 1.

\begin{figure}[h]
\caption{Architecture of the deep learning model with three dense layers}
\centering
\includegraphics[width=0.5\textwidth]{model_architecture.png}
\end{figure}

\begin{figure}[h]
\caption{Predictions of the model had 7\% accuracy}
\centering
\includegraphics[width=0.5\textwidth]{model_predictions.png}
\end{figure}

\section{Hierarchy of Multi-Disorders}

Since each disorder is represented by a Boolean value, and there are fourteen different labels, we may view the vector of labels as a single multi-label. These are identified with subsets of $2^14$. For example, suppose anxiety is the third disorder and ADHD is the fourth disorder, and someone is diagnosed with only these two disorders. Then the encoded vector would be $\left[0,0,1,1,0,0,0,0,0,0,0,0,0,0\right]$. Suppose depressive disorder is eighth. The vector would be $\left[0,0,1,1,0,0,0,1,0,0,0,0,0,0\right]$. In this case the double disorder ADHD+anxiety is a subset of the triple disorder ADHD+anxiety+depression. We can map all of these with this relationship. We find that some multi-disorders occur much more often than others, and keep only the top fifty for visualization purposes. Generally there are no more than five disorders per person reported. \\

It is particularly illuminating to see that some doubles and triples occur with low or zero frequency. This eliminates pathways to developing a multi-disorder, assuming someone would develop the syndrome with 2-disorders before developing the one with 3 containing it, etc.

\begin{figure}[h]
\caption{Hierarchy of multi-disorders, top 50}
\centering
\includegraphics[width=1.0\textwidth]{disorder_hierarchy_top50_labeled_legend.png}
\end{figure}

\section{Conclusion}

We explored and analyzed the data using a variety of techniques. To detect clustering, we used the unsupervised k-means method and found that it was best, revealing clear clusters when viewed through the lens of a t-SNE plot. \\

DBSCAN revealed essentially only the relative percentages of each multi-disorder. We learned that this is in fact a multilabel problem, as opposed to a mutliclass problem. However, one can phrase it as a multiclass problem by looking at the $2^14=16,384$ distinct multi-disorder classed. Those are reduced to about 400 actual observed multi-disorders, most of which are single, double, or at most triple diagnoses.  \\

The goal of this project was to do classfication, and we were able to use a variety of techniques including RandomForest, k-means, and logistic regression. It appears that RandomForest and LogisticRegression are both around 28\% accuracy and 19\% precision, which beats guessing the majority class which would be 16\% accuracy. We don't expect great classification with no symptom-level data, which would be a future direction for this project.

\end{document}  