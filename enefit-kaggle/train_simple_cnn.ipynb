{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528f8bb4-4dc6-4c65-83dd-fdf52856852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/jacksonwalters/Documents/GitHub/enefit-kaggle/predict-energy-behavior-of-prosumers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd1220bb-8699-4f87-a42e-9e065cd0ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from load_data import merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43775497-bc81-484a-ac68-9083ea3e16fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data...\n",
      "loading gas_prices...\n",
      "loading electricity_prices...\n",
      "loading forecast_weather...\n",
      "merging train and gas_prices...\n",
      "merging electricity_prices...\n",
      "merging forecast_weather...\n"
     ]
    }
   ],
   "source": [
    "#load the training data, dropping NaN's\n",
    "dataset = merged_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e62bd3-4c27-4eb5-b539-c9ea9c79501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce dataset to smaller size\n",
    "REDUCED_DATASET_SIZE = 1_000_000\n",
    "df = dataset[:REDUCED_DATASET_SIZE].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25b2195-efdf-4bf2-8b91-b61ae01236ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test-train-validation split on the data\n",
    "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "\n",
    "n = len(df)\n",
    "train_df = df[0:int(n*0.7)]\n",
    "val_df = df[int(n*0.7):int(n*0.9)]\n",
    "test_df = df[int(n*0.9):]\n",
    "\n",
    "num_features = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b14a60b0-59f4-4874-9037-74be638a44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write feature_names to .csv file\n",
    "import csv\n",
    "feature_names = list(df.columns.values)\n",
    "with open(\"../models/cnn_feature_names.csv\", 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce5e41ac-34a0-46c2-a250-2bcb3589ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle the indexes and offsets as shown in the diagrams above.\n",
    "#Split windows of features into (features, labels) pairs.\n",
    "#Plot the content of the resulting windows.\n",
    "#Efficiently generate batches of these windows from the training, evaluation, and test data, using tf.data.Datasets.\n",
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "def split_window(self, features):\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "  if self.label_columns is not None:\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window\n",
    "\n",
    "def make_dataset(self, data):\n",
    "  data = np.array(data, dtype=np.float32)\n",
    "  ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "      data=data,\n",
    "      targets=None,\n",
    "      sequence_length=self.total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=True,\n",
    "      batch_size=32,)\n",
    "\n",
    "  ds = ds.map(self.split_window)\n",
    "\n",
    "  return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset\n",
    "\n",
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.val_df)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_dataset(self.test_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.test))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57e0bcf4-8df7-4fea-984a-8f3fb237f6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==============================] - 20s 3ms/step - loss: 6482.7856 - mean_absolute_error: 2.7454\n"
     ]
    }
   ],
   "source": [
    "#define single step window\n",
    "single_step_window = WindowGenerator(\n",
    "    input_width=1, label_width=1, shift=1,\n",
    "    label_columns=['target'])\n",
    "single_step_window\n",
    "\n",
    "#define a baseline model\n",
    "class Baseline(tf.keras.Model):\n",
    "  def __init__(self, label_index=None):\n",
    "    super().__init__()\n",
    "    self.label_index = label_index\n",
    "\n",
    "  def call(self, inputs):\n",
    "    if self.label_index is None:\n",
    "      return inputs\n",
    "    result = inputs[:, :, self.label_index]\n",
    "    return result[:, :, tf.newaxis]\n",
    "\n",
    "baseline = Baseline(label_index=column_indices['target'])\n",
    "\n",
    "baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                 metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "#run the baseline model\n",
    "val_performance = {}\n",
    "performance = {}\n",
    "val_performance['Baseline'] = baseline.evaluate(single_step_window.val)\n",
    "performance['Baseline'] = baseline.evaluate(single_step_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "623acbd2-210b-4b84-999e-991d5c02029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the compile and fit training run\n",
    "MAX_EPOCHS = 1\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=.000_001),\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3aeb400-d06c-496a-82a1-65de075d921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the convolutional window\n",
    "CONV_WIDTH = 12\n",
    "conv_window = WindowGenerator(\n",
    "    input_width=CONV_WIDTH,\n",
    "    label_width=1,\n",
    "    shift=0,\n",
    "    label_columns=['target'])\n",
    "\n",
    "#convolutional neural network\n",
    "#for now inserting a BatchNormalization layer instead of normalizing manually\n",
    "conv_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv1D(filters=32,\n",
    "                           kernel_size=(CONV_WIDTH,),\n",
    "                           activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "    tf.keras.layers.Reshape(target_shape=()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe13bb6f-8602-4554-b3f7-a30e1ffcfa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv model on `conv_window`\n",
      "Input shape: (32, 12, 24)\n",
      "Output shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Conv model on `conv_window`\")\n",
    "print('Input shape:', conv_window.example[0].shape)\n",
    "print('Output shape:', conv_model(conv_window.example[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae3f166f-9939-4d6d-a3c0-1d0c44027d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_4 (Bat  (32, 12, 24)              96        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (32, 1, 32)               9248      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (32, 1, 32)               1056      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (32, 1, 1)                33        \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (32,)                     0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10433 (40.75 KB)\n",
      "Trainable params: 10385 (40.57 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38a768b5-5aca-4097-b112-7baa94d15be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21875/21875 [==============================] - 252s 11ms/step - loss: 387820.5938 - mean_absolute_error: 165.0231 - val_loss: 384226.3438 - val_mean_absolute_error: 164.8116\n",
      "6250/6250 [==============================] - 39s 6ms/step - loss: 384225.2812 - mean_absolute_error: 164.8123\n",
      "3125/3125 [==============================] - 20s 6ms/step - loss: 421606.0000 - mean_absolute_error: 174.1167\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "history = compile_and_fit(conv_model, conv_window)\n",
    "\n",
    "#evaluate performance\n",
    "val_performance['Conv'] = conv_model.evaluate(conv_window.val)\n",
    "performance['Conv'] = conv_model.evaluate(conv_window.test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b939ac2-7a75-4bea-9f03-5d528bf7f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "import pickle\n",
    "cnn_model_filename = '../models/cnn_model.sav'\n",
    "pickle.dump(conv_model, open(cnn_model_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b38ecdb-8588-4cc0-be49-1b4cc090160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model using keras\n",
    "conv_model.save('../models/cnn_model.keras')\n",
    "\n",
    "#save model using legacy keras\n",
    "conv_model.save('../models/cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f062499-4124-465f-a2e8-c4bd9f3e4680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([ 9.100809 ,  9.5893135, 10.810103 , 10.418977 , 10.649641 ,\n",
       "        7.31566  ,  8.783263 ,  9.060895 ,  9.563227 ,  8.855568 ,\n",
       "        7.5229936,  8.085756 ,  7.498831 , 11.097404 , 10.507688 ,\n",
       "        9.65713  ,  8.133643 ,  7.882827 , 10.019332 ,  9.624953 ,\n",
       "        9.987697 , 11.165125 ,  9.980533 , 11.019373 ,  8.992881 ,\n",
       "        9.189162 , 10.923996 ,  7.7113495,  7.044467 ,  9.912277 ,\n",
       "        9.421008 ,  9.990296 ], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model(conv_window.example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9195b32b-12aa-447c-9693-6a29f337289f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7.7927623, 6.436409 , 6.5068274, 5.9486175, 5.460173 , 7.3531957,\n",
       "       5.137743 , 5.2244844, 6.752815 , 6.929317 , 7.7764473, 6.024827 ,\n",
       "       5.8012033, 7.5677733, 6.253757 , 7.9643264, 6.1561403, 7.960485 ,\n",
       "       7.0253773, 7.8343205, 6.9852386, 6.8489156, 7.372115 , 8.234603 ,\n",
       "       5.6909256, 7.098112 , 7.8986735, 6.9408593, 7.2767396, 7.7127075,\n",
       "       7.123909 , 7.810176 ], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.predict(next(iter(conv_window.val))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "435fe662-d609-4adf-99c6-4e5b0f7883a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 9.100809 ,  9.5893135, 10.810103 , 10.418977 , 10.649641 ,\n",
       "        7.31566  ,  8.783263 ,  9.060895 ,  9.563227 ,  8.855568 ,\n",
       "        7.5229936,  8.085756 ,  7.498831 , 11.097404 , 10.507688 ,\n",
       "        9.65713  ,  8.133643 ,  7.882827 , 10.019332 ,  9.624953 ,\n",
       "        9.987697 , 11.165125 ,  9.980533 , 11.019373 ,  8.992881 ,\n",
       "        9.189162 , 10.923996 ,  7.7113495,  7.044467 ,  9.912277 ,\n",
       "        9.421008 ,  9.990296 ], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.predict(conv_window.example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80917033-c635-48b3-b3b9-252ca4ab9d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ML",
   "language": "python",
   "name": "venv-metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
